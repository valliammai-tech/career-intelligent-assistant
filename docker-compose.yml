services:
  api:
    build:
      context: .
      dockerfile: Dockerfile
      target: runtime
    container_name: career_intel_api
    ports:
      - "8000:8000"
    volumes:
      # Mount data folder so ChromaDB and uploads persist across restarts
      - ./data:/app/data
    environment:
      # On Windows Docker Desktop, host.docker.internal resolves to the Windows host
      # where Ollama is running as a native Windows service on port 11434
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
      - OLLAMA_LLM_MODEL=llama3.2:3b
      - OLLAMA_EMBED_MODEL=nomic-embed-text
      - CHROMA_PERSIST_DIR=/app/data/chroma_db
      - CHROMA_RESUME_COLLECTION=resume
      - CHROMA_JOBS_COLLECTION=jobs
      - RETRIEVAL_TOP_K=5
      - RETRIEVAL_MMR_LAMBDA=0.6
      - LLM_MAX_TOKENS=800
      - LLM_TEMPERATURE=0.1
      - CHUNK_SIZE=400
      - CHUNK_OVERLAP=80
      - CHUNK_MIN_TOKENS=50
      - API_HOST=0.0.0.0
      - API_PORT=8000
      - LOG_LEVEL=INFO
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/health')"]
      interval: 30s
      timeout: 10s
      start_period: 20s
      retries: 3
    restart: unless-stopped

  ui:
    build:
      context: .
      dockerfile: Dockerfile.streamlit
      target: runtime
    container_name: career_intel_ui
    ports:
      - "8501:8501"
    environment:
      # Inside Docker network, 'api' resolves to the API container
      - API_URL=http://api:8000
    depends_on:
      api:
        condition: service_healthy
    restart: unless-stopped
